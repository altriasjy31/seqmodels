{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seqmodels",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "t1p_laN0Y0DH",
        "D1dhVxJxY7C4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/seqmodels/blob/master/seqmodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1dhVxJxY7C4",
        "colab_type": "text"
      },
      "source": [
        "## inv_cov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV7mvA6GZ9y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def tf_cov(x,w=None,do_mean=True):\n",
        "  if w is None:\n",
        "    num_points = tf.cast(tf.shape(x)[0],tf.float32) - 1.0\n",
        "    if do_mean:\n",
        "      x_mean = tf.reduce_mean(x, axis=0, keep_dims=True)\n",
        "      x = (x - x_mean)\n",
        "  else:\n",
        "    num_points = tf.reduce_sum(w) - tf.sqrt(tf.reduce_mean(w))\n",
        "    if do_mean:\n",
        "      x_mean = tf.reduce_sum(x * w[:,None], axis=0, keepdims=True) / num_points\n",
        "      x = (x - x_mean) * tf.sqrt(w[:,None])\n",
        "    else:\n",
        "      x = x * tf.sqrt(w[:,None])\n",
        "  return tf.matmul(tf.transpose(x),x)/num_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCMPaTvVY9__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def INV_COV(X,W,pcc=False):\n",
        "  tf.reset_default_graph()\n",
        "  \n",
        "  x = tf.placeholder(tf.uint8,shape=(None,None),name=\"x\")\n",
        "  x_weights = tf.placeholder(tf.float32,shape=(None,),name=\"x_weights\") \n",
        "  \n",
        "  x_shape = tf.shape(x)\n",
        "  x_nr = x_shape[0]\n",
        "  x_nc = x_shape[1]\n",
        "\n",
        "  x_msa = tf.one_hot(x,21)\n",
        "\n",
        "  cat = 21\n",
        "  x_en = x_msa    \n",
        "  x_feat = tf.reshape(x_en,(x_nr, x_nc * cat))\n",
        "\n",
        "  # compute covariance\n",
        "  x_c = tf_cov(x_feat,x_weights,do_mean=do_mean)\n",
        "\n",
        "  # add reg\n",
        "  x_reg_b = 4.5\n",
        "  x_reg_alpha = x_reg_b/tf.sqrt(tf.reduce_sum(x_weights))\n",
        "  x_c = x_c + x_reg_alpha * tf.eye(x_nc * cat)\n",
        "  \n",
        "  # compute inverse\n",
        "  x_c_inv = tf.linalg.inv(x_c)\n",
        "  \n",
        "  if pcc:\n",
        "    # partial correlation coefficient\n",
        "    x_c_inv_diag = tf.linalg.diag_part(x_c_inv)\n",
        "    x_c_inv = x_c_inv / tf.sqrt(x_c_inv_diag[None,:] * x_c_inv_diag[:,None])\n",
        "    \n",
        "  # get w matrix\n",
        "  x_w = tf.reshape(x_c_inv,shape=(x_nc,cat,x_nc,cat))\n",
        "  x_w = x_w[:,:-1,:,:-1]\n",
        "  x_wi = tf.sqrt(tf.reduce_sum(tf.square(x_w),axis=(1,3)))\n",
        "  if zero:\n",
        "    x_wi = x_wi * (1.0 - tf.eye(x_nc))\n",
        "    \n",
        "  # do apc\n",
        "  x_ap_sum = tf.reduce_sum(x_wi,axis=0)\n",
        "  x_ap = x_ap_sum[None,:] * x_ap_sum[:,None] / tf.reduce_sum(x_ap_sum)\n",
        "  x_wip = (x_wi - x_ap) \n",
        "  x_wip = x_wip * (1.0 - tf.eye(x_nc))\n",
        "  \n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(200):\n",
        "      sess.run(opt)\n",
        "      if ((i+1) % 100) == 0:\n",
        "        print((i+1),sess.run(loss))\n",
        "    return sess.run(x_wip,{x:X,x_weights:W})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdF3aG3jMC0a",
        "colab_type": "text"
      },
      "source": [
        "## DI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IEKwuDaMCRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import squareform\n",
        "def get_di(X,W,pc=0.5,k_max=10):\n",
        "  \n",
        "  nr = X.shape[0]\n",
        "  nc = X.shape[1]\n",
        "  ns = 21\n",
        "  na = ns-1   \n",
        "  \n",
        "  x = np.eye(21)[X]\n",
        "\n",
        "  ni, nj = np.triu_indices(nc,k=1)\n",
        "  nw = len(ni)\n",
        "\n",
        "  f = np.sum(x * W[:,None,None],0) / np.sum(W)\n",
        "  x_feat = np.reshape(x,(nr,nc*ns))\n",
        "  x_feat = x_feat * np.sqrt(W[:,None])\n",
        "  c = np.matmul(np.transpose(x_feat), x_feat)/np.sum(W)\n",
        "  c = c.reshape(nc,ns,nc,ns)\n",
        "\n",
        "  ff = c[ni,:,nj,:]\n",
        "  ff_diag = c[range(nc),:,range(nc)]\n",
        "  \n",
        "  # add pseudocount\n",
        "  if pc > 0:\n",
        "    pc_i = pc / ns\n",
        "    pc_ij = pc / (ns * ns)\n",
        "\n",
        "    f = (1-pc) * f + pc_i\n",
        "    ff = (1-pc) * ff + pc_ij\n",
        "    ff_diag = (1-pc) * ff_diag + pc_i * np.eye(ns)\n",
        "\n",
        "  # compute covariance matrix\n",
        "  c = ff[:,:-1,:-1] - f[ni,:-1,None] * f[nj,None,:-1]\n",
        "  c_diag = ff_diag[:,:-1,:-1] - f[:,:-1,None] * f[:,None,:-1]\n",
        "\n",
        "  # make covariance matrix into square\n",
        "  c_sq = np.zeros((nc,na,nc,na))\n",
        "  c_sq[ni,:,nj,:] = c\n",
        "  c_sq[nj,:,ni,:] = c.transpose((0,2,1))\n",
        "  c_sq[range(nc),:,range(nc),:] = c_diag\n",
        "\n",
        "  # invert covariance matrix\n",
        "  c_inv = np.linalg.inv(c_sq.reshape(nc*na,nc*na)).reshape(nc,na,nc,na)[ni,:,nj,:]\n",
        "\n",
        "  w = np.ones((nw,ns,ns))\n",
        "  w[:,:-1,:-1] = np.exp(-c_inv)\n",
        "\n",
        "  # recompute entropy (needed for di)\n",
        "  h = -np.sum(f * np.log(f + 1e-8),1)\n",
        "\n",
        "  # compute di\n",
        "  di = np.empty(nw)\n",
        "  for n,(i,j) in enumerate(zip(ni,nj)):\n",
        "\n",
        "    vi = np.ones(ns)/ns\n",
        "    vj = np.ones(ns)/ns\n",
        "    for k in range(k_max):\n",
        "      vi = f[i] / np.einsum(\"j,ij -> i\",vj,w[n])\n",
        "      vi = vi / np.sum(vi)\n",
        "\n",
        "      vj = f[j] / np.einsum(\"i,ij -> j\",vi,w[n])\n",
        "      vj = vj / np.sum(vj)\n",
        "\n",
        "    ff_di = w[n] * vi[:,None] * vj[None,:]\n",
        "    ff_di = ff_di / np.sum(ff_di)\n",
        "\n",
        "    di[n] = h[i] + h[j] + np.sum(ff_di * np.log(ff_di + 1e-8))\n",
        "\n",
        "  di = squareform(di)\n",
        "  ap = np.sum(di,0)\n",
        "  return di - (ap[None,:] * ap[:,None]) / np.sum(ap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvoLDggl5aL8",
        "colab_type": "text"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xZuDVhHg5Kn-",
        "colab": {}
      },
      "source": [
        "def opt_adam(loss, name, var_list=None, lr=1.0, b1=0.9, b2=0.999, b_fix=False):\n",
        "  \n",
        "  if var_list is None: var_list = tf.trainable_variables() \n",
        "  gradients = tf.gradients(loss,var_list)\n",
        "  if b_fix: t = tf.Variable(0.0,\"t\")\n",
        "  opt = []\n",
        "  for n,(x,g) in enumerate(zip(var_list,gradients)):\n",
        "    if g is not None:\n",
        "      ini = dict(initializer=tf.zeros_initializer,trainable=False)\n",
        "      mt = tf.get_variable(name+\"_mt_\"+str(n),shape=list(x.shape), **ini)\n",
        "      vt = tf.get_variable(name+\"_vt_\"+str(n),shape=[], **ini)\n",
        "      \n",
        "      mt_tmp = b1*mt+(1-b1)*g\n",
        "      vt_tmp = b2*vt+(1-b2)*tf.reduce_sum(tf.square(g))\n",
        "      lr_tmp = lr/(tf.sqrt(vt_tmp) + 1e-8)\n",
        "\n",
        "      if b_fix: lr_tmp = lr_tmp * tf.sqrt(1-tf.pow(b2,t))/(1-tf.pow(b1,t))\n",
        "\n",
        "      opt.append(x.assign_add(-lr_tmp * mt_tmp))\n",
        "      opt.append(vt.assign(vt_tmp))\n",
        "      opt.append(mt.assign(mt_tmp))\n",
        "        \n",
        "  if b_fix: opt.append(t.assign_add(1.0))\n",
        "  return(tf.group(opt))\n",
        "\n",
        "def sym_w(w,w_zero=True):\n",
        "  '''symmetrize input matrix of shape (x,y,x,y)'''\n",
        "  x = w.shape[0]\n",
        "  if w_zero:\n",
        "    w = w * np.reshape(1-np.eye(x),(x,1,x,1))\n",
        "  w = w + tf.transpose(w,[2,3,0,1])\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X1VJiPzJ5KoQ"
      },
      "source": [
        "## MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gIRz-XC5yyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def L2(x, axis=None):\n",
        "  return tf.reduce_sum(tf.square(x),axis=axis)\n",
        "\n",
        "def MSE(X,W,lam_w=4.5):  \n",
        "  \n",
        "  tf.reset_default_graph()\n",
        "  ln = X.shape[1]\n",
        "  fs = ln * 21\n",
        "\n",
        "  x = tf.one_hot(tf.constant(X,tf.uint8),21)  \n",
        "  x = tf.reshape(x,(-1,fs))  \n",
        "  \n",
        "  x_weights = tf.constant(W,tf.float32)\n",
        "      \n",
        "  w = tf.get_variable(\"w\",shape=(fs,fs),initializer=tf.zeros_initializer)\n",
        "  \n",
        "    x_mean = tf.reduce_mean(x, axis=0, keepdims=True)\n",
        "    y = tf.tensordot(x - x_mean, w, 1) + x_mean\n",
        "  \n",
        "  \n",
        "  loss = tf.reduce_sum(L2(x-y,-1) * x_weights) / tf.reduce_sum(x_weights)\n",
        "  loss = loss + L2(w-tf.transpose(w))\n",
        "  \n",
        "  if trace:\n",
        "    loss_trace = -1.0 * tf.trace(w)\n",
        "    loss = loss + loss_trace\n",
        "\n",
        "  x_w = tf.reshape(w,(ln,21,ln,21))\n",
        "  \n",
        "  if lam_w > 0:\n",
        "    loss_reg = (lam_w/tf.sqrt(tf.reduce_sum(x_weights))) * L2(w)\n",
        "    loss = loss + loss_reg    \n",
        "  \n",
        "  x_wi = tf.sqrt(tf.reduce_sum(tf.square(x_w[:,:-1,:,:-1]), axis=(1,3)))\n",
        "  x_wi = x_wi * (1-tf.eye(ln))\n",
        "  \n",
        "  x_wi = 0.5 * (x_wi + tf.transpose(x_wi))\n",
        "  x_ap_sum = tf.reduce_sum(x_wi,axis=0)\n",
        "  x_ap = x_ap_sum[None,:] * x_ap_sum[:,None] / tf.reduce_sum(x_ap_sum)\n",
        "  x_wip = (x_wi - x_ap)\n",
        "  x_wip = x_wip * (1-tf.eye(ln))\n",
        "  \n",
        "  opt = opt_adam(loss,\"opt\",lr=1.0)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(200):\n",
        "      sess.run(opt)\n",
        "      if ((i+1) % 100) == 0:\n",
        "        print((i+1),sess.run(loss))\n",
        "    return sess.run(x_wip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQP7M_58541c",
        "colab_type": "text"
      },
      "source": [
        "## CCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7sncFlP5eVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CCE(msa,\n",
        "            msa_weights=None,\n",
        "            opt_rate=0.5,\n",
        "            opt_iter=100,\n",
        "            use_bias=True,\n",
        "            lam_w=0.01,\n",
        "            lam_b=0.01,\n",
        "            ignore_gap=False):\n",
        "\n",
        "  # collecting some information about input msa\n",
        "  nrow = msa.shape[0] # number of sequences\n",
        "  ncol = msa.shape[1] # length of sequence\n",
        "  if ignore_gap:\n",
        "    ncat = 20\n",
        "  else:\n",
        "    ncat = 21\n",
        "  \n",
        "  if msa_weights is None:\n",
        "    msa_weights = np.ones(nrow)\n",
        " \n",
        "  \n",
        "  # kill any existing tensorflow graph\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # setting up weights\n",
        "  if use_bias:\n",
        "    b = tf.get_variable(\"b\", [ncol,ncat],\n",
        "                        initializer=tf.initializers.zeros)\n",
        "    \n",
        "  w = tf.get_variable(\"w\", [ncol,ncat,ncol,ncat],\n",
        "                      initializer=tf.initializers.zeros)\n",
        "  # symmetrize w\n",
        "  w = sym_w(w)\n",
        "  \n",
        "  # input\n",
        "  X = tf.constant(msa,tf.uint8,name=\"x\")\n",
        "  MSA = tf.one_hot(X,21)\n",
        "  if ignore_gap:\n",
        "    non_gap = tf.abs(1.0 - MSA[...,-1])\n",
        "    MSA = MSA[...,:-1]\n",
        "  \n",
        "    \n",
        "  MSA_weights = tf.constant(msa_weights,tf.float32,name=\"msa_weights\")\n",
        "  NEFF = tf.reduce_sum(MSA_weights)\n",
        "\n",
        "  # dense layer + softmax activation\n",
        "  a = tf.tensordot(MSA,w,2)\n",
        "\n",
        "  if use_bias:\n",
        "    MSA_logit = a + b\n",
        "  else:\n",
        "    MSA_logit = a\n",
        "    \n",
        "  MSA_pred = tf.nn.softmax(MSA_logit, -1)\n",
        "\n",
        "  # loss = categorical crossentropy (aka pseudo-likelihood)\n",
        "  loss = tf.keras.losses.categorical_crossentropy(MSA,MSA_pred)\n",
        "  if ignore_gap:\n",
        "    loss = loss * non_gap\n",
        "    \n",
        "  loss = tf.reduce_sum(loss, axis=-1)\n",
        "  loss = tf.reduce_sum(loss * MSA_weights)/NEFF\n",
        "\n",
        "  # add L2 regularization\n",
        "  reg_w = lam_w * tf.reduce_sum(tf.square(w)) * 0.5 * (ncol-1) * 20.0 \n",
        "  if use_bias:\n",
        "    reg_b = lam_b * tf.reduce_sum(tf.square(b))\n",
        "    loss = loss + (reg_b + reg_w)/NEFF\n",
        "  else:\n",
        "    loss = loss + (reg_w)/NEFF\n",
        "\n",
        "  opt = opt_adam(loss,\"adam\", lr=opt_rate)\n",
        "  x_w = w\n",
        "  \n",
        "  # l2norm\n",
        "  if ignore_gap is False:\n",
        "    x_w = x_w[:,:-1,:,:-1]\n",
        "  \n",
        "  x_wi = tf.sqrt(tf.reduce_sum(tf.square(x_w), axis=(1,3))) * (1-tf.eye(ncol))\n",
        "  \n",
        "  # do apc\n",
        "  x_ap_sum = tf.reduce_sum(x_wi,axis=0)\n",
        "  x_ap = x_ap_sum[None,:] * x_ap_sum[:,None] / tf.reduce_sum(x_ap_sum)\n",
        "  x_wip = (x_wi - x_ap) * (1-tf.eye(ncol))\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    if use_bias:\n",
        "      msa_cat = np.eye(21)[msa]\n",
        "      if ignore_gap:\n",
        "        msa_cat = msa_cat[...,:-1]\n",
        "      pseudo_count = 0.01 * np.log(np.sum(msa_weights))\n",
        "      b_ini = np.log(np.sum(msa_cat.T * msa_weights,-1).T + pseudo_count)\n",
        "      b_ini = b_ini - np.mean(b_ini,-1,keepdims=True)\n",
        "      sess.run(b.assign(b_ini))\n",
        "    \n",
        "    for k in range(opt_iter):\n",
        "      sess.run(opt)\n",
        "      #if (k+1) % int(opt_iter/10) == 0:\n",
        "      #  print((k+1),sess.run(loss))\n",
        "      \n",
        "    return sess.run(x_wip)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}